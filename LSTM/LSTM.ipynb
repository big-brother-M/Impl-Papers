{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치(PyTorch)로 구현하는 LSTM 언어 모델\n",
    "\n",
    "본 노트북은 **\"Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling\" (Sak et al., 2014)** 논문을 기반으로 LSTM(Long Short-Term Memory) 네트워크를 구현합니다.\n",
    "\n",
    "LSTM은 기존 RNN의 장기 의존성(Long-term Dependency) 문제를 해결하기 위해 설계된 특수한 RNN 아키텍처입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 목차\n",
    "\n",
    "1. [LSTM 이론 및 구조](#1-lstm-이론-및-구조)\n",
    "2. [환경 설정](#2-환경-설정)\n",
    "3. [데이터 로드 및 전처리](#3-데이터-로드-및-전처리)\n",
    "4. [LSTM 모델 구현](#4-lstm-모델-구현)\n",
    "5. [학습](#5-학습-training)\n",
    "6. [시각화 및 결과 분석](#6-결과-시각화)\n",
    "7. [텍스트 생성](#7-텍스트-생성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM 이론 및 구조\n",
    "\n",
    "### 1.1 RNN의 한계: 기울기 소실 문제\n",
    "\n",
    "기존 RNN은 시퀀스가 길어질수록 **기울기 소실(Vanishing Gradient)** 또는 **기울기 폭발(Exploding Gradient)** 문제가 발생합니다. 이로 인해 먼 과거의 정보를 현재 시점으로 전달하기 어렵습니다.\n",
    "\n",
    "### 1.2 LSTM의 핵심 아이디어\n",
    "\n",
    "LSTM은 **메모리 셀(Memory Cell)**과 **게이트(Gate)** 메커니즘을 도입하여 이 문제를 해결합니다:\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│                     LSTM Memory Block                      │\n",
    "│                                                            │\n",
    "│   ┌─────┐    ┌─────┐    ┌─────┐                            │\n",
    "│   │  f  │    │  i  │    │  o  │    ← 게이트들                │\n",
    "│   │ 망각 │    │ 입력 │    │ 출력 │                            │\n",
    "│   └──┬──┘    └──┬──┘    └──┬──┘                            │\n",
    "│      │          │          │                               │\n",
    "│      ▼          ▼          ▼                               │\n",
    "│   ┌──────────────────────────┐                             │\n",
    "│   │      Memory Cell (c)     │ ← 장기 메모리                  │\n",
    "│   └──────────────────────────┘                             │\n",
    "│                  │                                         │\n",
    "│                  ▼                                         │\n",
    "│            [Hidden State h]    ← 단기 메모리/출력              │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 1.3 LSTM 게이트 상세 설명\n",
    "\n",
    "#### 망각 게이트 (Forget Gate)\n",
    "- **역할**: 이전 셀 상태에서 어떤 정보를 \"잊을지\" 결정\n",
    "- **수식**: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "- **출력**: 0~1 사이 값 (0: 완전히 잊음, 1: 완전히 유지)\n",
    "\n",
    "#### 입력 게이트 (Input Gate)\n",
    "- **역할**: 새로운 정보 중 어떤 것을 셀 상태에 \"저장할지\" 결정\n",
    "- **수식**: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "- **후보 값**: $\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$\n",
    "\n",
    "#### 셀 상태 업데이트 (Cell State Update)\n",
    "- **역할**: 이전 정보와 새 정보를 조합하여 장기 메모리 갱신\n",
    "- **수식**: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n",
    "\n",
    "#### 출력 게이트 (Output Gate)\n",
    "- **역할**: 셀 상태에서 어떤 정보를 \"출력할지\" 결정\n",
    "- **수식**: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "- **은닉 상태**: $h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "### 1.4 왜 LSTM이 장기 의존성을 학습할 수 있는가?\n",
    "\n",
    "1. **셀 상태의 직접적인 경로**: $c_t = f_t \\odot c_{t-1} + ...$에서 $f_t \\approx 1$이면 기울기가 거의 그대로 전파\n",
    "2. **덧셈 연결**: 곱셈이 아닌 덧셈으로 정보 결합 → 기울기 소실 완화\n",
    "3. **게이트 메커니즘**: 적응적으로 정보 흐름 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 재현 가능성을 위한 시드 고정\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device 설정\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"사용 가능한 최적의 디바이스 반환\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"사용 장치(Device): {device}\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"모델 및 학습 하이퍼파라미터 설정\"\"\"\n",
    "    \n",
    "    # 데이터 설정\n",
    "    dataset_name: str = \"Salesforce/wikitext\"\n",
    "    dataset_config: str = \"wikitext-2-raw-v1\"\n",
    "    min_count: int = 5  # 최소 출현 빈도\n",
    "    \n",
    "    # 모델 하이퍼파라미터\n",
    "    embed_size: int = 256      # 임베딩 차원\n",
    "    hidden_size: int = 256     # LSTM 은닉 상태 차원\n",
    "    num_layers: int = 2        # LSTM 레이어 수\n",
    "    dropout: float = 0.3       # 드롭아웃 비율\n",
    "    \n",
    "    # 학습 설정\n",
    "    batch_size: int = 20\n",
    "    time_size: int = 35        # Truncated BPTT 길이\n",
    "    max_epoch: int = 10        # 충분한 학습\n",
    "    learning_rate: float = 0.001  # Adam 학습률\n",
    "    clip_grad: float = 5.0     # Gradient Clipping\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Config(embed_size={self.embed_size}, hidden_size={self.hidden_size}, \"\n",
    "            f\"num_layers={self.num_layers}, dropout={self.dropout}, \"\n",
    "            f\"batch_size={self.batch_size}, time_size={self.time_size}, \"\n",
    "            f\"max_epoch={self.max_epoch}, lr={self.learning_rate})\"\n",
    "        )\n",
    "\n",
    "config = Config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드 및 전처리\n",
    "\n",
    "**Wikitext-2** 데이터셋을 사용합니다. RNN 노트북과 동일한 전처리 과정을 거칩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"데이터셋 다운로드 및 로드 중...\")\n",
    "ds = load_dataset(config.dataset_name, config.dataset_config)\n",
    "print(f\"데이터셋 로드 완료!\")\n",
    "print(f\"   - Train: {len(ds['train'])} 문장\")\n",
    "print(f\"   - Validation: {len(ds['validation'])} 문장\")\n",
    "print(f\"   - Test: {len(ds['test'])} 문장\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(\n",
    "    dataset, \n",
    "    splits: List[str] = ['train', 'validation', 'test'],\n",
    "    min_count: int = 5\n",
    ") -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    데이터셋에서 단어 빈도를 카운팅하고 어휘 사전을 구축합니다.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    for split in splits:\n",
    "        for line in dataset[split]['text']:\n",
    "            words = line.strip().lower().split()\n",
    "            counter.update(words)\n",
    "    \n",
    "    # <unk> 토큰은 항상 0번 ID\n",
    "    word_to_id = {'<unk>': 0}\n",
    "    id_to_word = {0: '<unk>'}\n",
    "    \n",
    "    # 빈도순으로 정렬하여 ID 부여\n",
    "    sorted_words = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    valid_words = [word for word, count in sorted_words if count >= min_count]\n",
    "    \n",
    "    for word in valid_words:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "# 어휘 사전 구축\n",
    "word_to_id, id_to_word = build_vocab(ds, min_count=config.min_count)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "print(f\"\\n어휘 사전 통계:\")\n",
    "print(f\"   - 전체 어휘 크기: {vocab_size:,}\")\n",
    "print(f\"   - 최소 출현 빈도: {config.min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(\n",
    "    dataset, \n",
    "    split: str, \n",
    "    word_to_id: Dict[str, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 단어 ID 시퀀스로 변환합니다.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    unk_id = word_to_id['<unk>']\n",
    "    \n",
    "    for line in dataset[split]['text']:\n",
    "        words = line.strip().lower().split()\n",
    "        if not words:\n",
    "            continue\n",
    "        ids.extend([word_to_id.get(w, unk_id) for w in words])\n",
    "    \n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "# 각 분할에 대해 ID 변환\n",
    "corpus_train = convert_to_ids(ds, 'train', word_to_id)\n",
    "corpus_valid = convert_to_ids(ds, 'validation', word_to_id)\n",
    "corpus_test = convert_to_ids(ds, 'test', word_to_id)\n",
    "\n",
    "print(f\"\\n코퍼스 크기:\")\n",
    "print(f\"   - Train: {len(corpus_train):,} tokens\")\n",
    "print(f\"   - Validation: {len(corpus_valid):,} tokens\")\n",
    "print(f\"   - Test: {len(corpus_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM 모델 구현\n",
    "\n",
    "논문의 수식을 직접 구현한 **커스텀 LSTM 셀**과, PyTorch의 `nn.LSTM`을 활용한 언어 모델을 구현합니다.\n",
    "\n",
    "### 4.1 커스텀 LSTM 셀 (교육용)\n",
    "\n",
    "LSTM의 내부 동작을 이해하기 위해 직접 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    커스텀 LSTM 셀 구현 (Peephole Connections 포함).\n",
    "    \n",
    "    Hochreiter & Schmidhuber 1997 논문의 수식을 충실히 구현:\n",
    "        i_t = σ(W_ix * x_t + W_ih * h_{t-1} + W_ci * c_{t-1} + b_i)  # 입력 게이트\n",
    "        f_t = σ(W_fx * x_t + W_fh * h_{t-1} + W_cf * c_{t-1} + b_f)  # 망각 게이트  \n",
    "        g_t = tanh(W_gx * x_t + W_gh * h_{t-1} + b_g)  # 후보 셀 상태\n",
    "        c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t  # 셀 상태\n",
    "        o_t = σ(W_ox * x_t + W_oh * h_{t-1} + W_co * c_t + b_o)  # 출력 게이트\n",
    "        h_t = o_t ⊙ tanh(c_t)  # 은닉 상태\n",
    "    \n",
    "    Peephole connections: 셀 상태(c)가 게이트들에 직접 연결되어\n",
    "    더 정밀한 정보 흐름 제어가 가능합니다.\n",
    "    \n",
    "    Args:\n",
    "        input_size: 입력 차원\n",
    "        hidden_size: 은닉 상태 차원\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 4개 게이트를 하나의 행렬로 통합 (효율성)\n",
    "        # 순서: [input_gate, forget_gate, cell_gate, output_gate]\n",
    "        self.W_x = nn.Linear(input_size, 4 * hidden_size, bias=False)\n",
    "        self.W_h = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
    "        \n",
    "        # Peephole 가중치 (대각 행렬을 벡터로 표현 - 메모리 효율적)\n",
    "        self.W_ci = nn.Parameter(torch.zeros(hidden_size))  # 입력 게이트용\n",
    "        self.W_cf = nn.Parameter(torch.zeros(hidden_size))  # 망각 게이트용\n",
    "        self.W_co = nn.Parameter(torch.zeros(hidden_size))  # 출력 게이트용\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"개선된 가중치 초기화\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_x' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'W_h' in name and 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                # 망각 게이트 편향은 1로 초기화 (초기에 정보 유지 유도)\n",
    "                n = param.size(0)\n",
    "                param.data[n//4:n//2].fill_(1.0)\n",
    "        \n",
    "        # Peephole 가중치는 작은 값으로 초기화\n",
    "        nn.init.uniform_(self.W_ci, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_cf, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_co, -0.1, 0.1)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        hx: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        순전파 계산.\n",
    "        \n",
    "        Args:\n",
    "            x: 입력 (batch, input_size)\n",
    "            hx: (h_prev, c_prev) 튜플, None이면 0으로 초기화\n",
    "            \n",
    "        Returns:\n",
    "            h_next: 새로운 은닉 상태 (batch, hidden_size)\n",
    "            c_next: 새로운 셀 상태 (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 초기 상태가 없으면 0으로 초기화\n",
    "        if hx is None:\n",
    "            h_prev = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "            c_prev = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            h_prev, c_prev = hx\n",
    "        \n",
    "        # 4개 게이트 한번에 계산 (기본 연산)\n",
    "        gates = self.W_x(x) + self.W_h(h_prev)  # (batch, 4*hidden)\n",
    "        \n",
    "        # 게이트 분리\n",
    "        H = self.hidden_size\n",
    "        i_gate_pre = gates[:, :H]\n",
    "        f_gate_pre = gates[:, H:2*H]\n",
    "        g_gate_pre = gates[:, 2*H:3*H]\n",
    "        o_gate_pre = gates[:, 3*H:]\n",
    "        \n",
    "        # Peephole connections 적용 (c_{t-1}이 입력/망각 게이트에 영향)\n",
    "        i_gate = torch.sigmoid(i_gate_pre + self.W_ci * c_prev)  # 입력 게이트\n",
    "        f_gate = torch.sigmoid(f_gate_pre + self.W_cf * c_prev)  # 망각 게이트\n",
    "        g_gate = torch.tanh(g_gate_pre)  # 후보 셀 상태\n",
    "        \n",
    "        # 셀 상태 업데이트\n",
    "        c_next = f_gate * c_prev + i_gate * g_gate\n",
    "        \n",
    "        # 출력 게이트 (c_t가 영향) - Peephole\n",
    "        o_gate = torch.sigmoid(o_gate_pre + self.W_co * c_next)\n",
    "        \n",
    "        # 은닉 상태 계산\n",
    "        h_next = o_gate * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    커스텀 LSTM 레이어 - 시퀀스 전체를 처리.\n",
    "    \n",
    "    Args:\n",
    "        input_size: 입력 차원\n",
    "        hidden_size: 은닉 상태 차원\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        hx: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        순전파 계산.\n",
    "        \n",
    "        Args:\n",
    "            x: 입력 시퀀스 (batch, time, input_size)\n",
    "            hx: 초기 (h, c) 상태, None이면 0으로 초기화\n",
    "            \n",
    "        Returns:\n",
    "            outputs: 모든 시간 단계의 은닉 상태 (batch, time, hidden_size)\n",
    "            (h_n, c_n): 마지막 은닉/셀 상태\n",
    "        \"\"\"\n",
    "        batch_size, time_size, _ = x.shape\n",
    "        \n",
    "        # 초기 상태 설정\n",
    "        if hx is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            h, c = hx\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(time_size):\n",
    "            h, c = self.cell(x[:, t, :], (h, c))\n",
    "            outputs.append(h.unsqueeze(1))\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, time, hidden)\n",
    "        return outputs, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LSTM 언어 모델\n",
    "\n",
    "실제 학습에는 PyTorch의 최적화된 `nn.LSTM`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM 기반 언어 모델.\n",
    "    \n",
    "    구조: Embedding → Dropout → LSTM → Dropout → Linear\n",
    "    \n",
    "    Hochreiter & Schmidhuber 1997 논문 기반 구현.\n",
    "    Weight Tying 및 개선된 가중치 초기화 적용.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 어휘 크기\n",
    "        embed_size: 임베딩 차원\n",
    "        hidden_size: LSTM 은닉 상태 차원\n",
    "        num_layers: LSTM 레이어 수\n",
    "        dropout: 드롭아웃 비율\n",
    "        tie_weights: 임베딩-출력 가중치 공유 여부\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        embed_size: int, \n",
    "        hidden_size: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        tie_weights: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 1. 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # 2. 드롭아웃\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # 3. LSTM 레이어 (다층)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 4. 출력 레이어\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Weight Tying: 임베딩과 출력층 가중치 공유\n",
    "        if tie_weights and embed_size == hidden_size:\n",
    "            self.fc.weight = self.embedding.weight\n",
    "        \n",
    "        # 개선된 가중치 초기화\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self, init_range: float = 0.1):\n",
    "        \"\"\"LSTM에 최적화된 가중치 초기화\"\"\"\n",
    "        # 임베딩 초기화\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "        # LSTM 가중치 초기화\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                # 입력-은닉 가중치: Xavier 초기화\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                # 은닉-은닉 가중치: 직교 초기화\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                # 망각 게이트 편향을 1로 초기화 (정보 유지 장려)\n",
    "                n = param.size(0)\n",
    "                param.data[n//4:n//2].fill_(1.0)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        순전파.\n",
    "        \n",
    "        Args:\n",
    "            x: 입력 단어 ID (batch, time_size)\n",
    "            hidden: LSTM 초기 상태 (h, c)\n",
    "            \n",
    "        Returns:\n",
    "            output: 로짓 (batch, time_size, vocab_size)\n",
    "            hidden: 새로운 LSTM 상태\n",
    "        \"\"\"\n",
    "        # 임베딩\n",
    "        embeds = self.drop(self.embedding(x))  # (batch, time, embed)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)  # (batch, time, hidden)\n",
    "        \n",
    "        # 출력층\n",
    "        output = self.drop(lstm_out)\n",
    "        output = self.fc(output)  # (batch, time, vocab)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"은닉 상태 초기화\"\"\"\n",
    "        weight = next(self.parameters())\n",
    "        h = weight.new_zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c = weight.new_zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return (h, c)\n",
    "    \n",
    "    def get_num_params(self) -> int:\n",
    "        \"\"\"학습 가능한 파라미터 수 반환\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=config.embed_size,\n",
    "    hidden_size=config.hidden_size,\n",
    "    num_layers=config.num_layers,\n",
    "    dropout=config.dropout,\n",
    "    tie_weights=True\n",
    ").to(device)\n",
    "\n",
    "print(\"모델 구조:\")\n",
    "print(model)\n",
    "print(f\"\\n총 파라미터 수: {model.get_num_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 (Training)\n",
    "\n",
    "### Truncated BPTT\n",
    "RNN/LSTM의 역전파는 시간을 거슬러 올라가며 수행됩니다. 전체 시퀀스에 대해 BPTT를 수행하면 비용이 크므로 일정 길이로 잘라서 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    corpus: np.ndarray,\n",
    "    batch_size: int,\n",
    "    time_size: int,\n",
    "    time_idx: int,\n",
    "    offsets: List[int],\n",
    "    data_size: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    학습용 미니배치 생성.\n",
    "    \"\"\"\n",
    "    batch_x, batch_t = [], []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        ptr = (offsets[i] + time_idx) % data_size\n",
    "        indices = [(ptr + t) % data_size for t in range(time_size + 1)]\n",
    "        raw_data = corpus[indices]\n",
    "        batch_x.append(raw_data[:-1])\n",
    "        batch_t.append(raw_data[1:])\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(np.array(batch_x), dtype=torch.long),\n",
    "        torch.tensor(np.array(batch_t), dtype=torch.long)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    corpus: np.ndarray,\n",
    "    batch_size: int,\n",
    "    time_size: int,\n",
    "    criterion: nn.Module\n",
    ") -> float:\n",
    "    \"\"\"모델 평가하고 평균 손실 반환\"\"\"\n",
    "    model.eval()\n",
    "    data_size = len(corpus)\n",
    "    max_iters = data_size // (batch_size * time_size)\n",
    "    jump = (data_size - 1) // batch_size\n",
    "    offsets = [i * jump for i in range(batch_size)]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iter_idx in range(max_iters):\n",
    "            inputs, targets = get_batch(\n",
    "                corpus, batch_size, time_size, \n",
    "                iter_idx * time_size, offsets, data_size\n",
    "            )\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, model.vocab_size),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if hidden is not None:\n",
    "                hidden = tuple(h.detach() for h in hidden)\n",
    "    \n",
    "    return total_loss / max_iters if max_iters > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "# 배치 샘플링을 위한 오프셋 계산\n",
    "data_size = len(corpus_train)\n",
    "max_iters = data_size // (config.batch_size * config.time_size)\n",
    "jump = (data_size - 1) // config.batch_size\n",
    "offsets = [i * jump for i in range(config.batch_size)]\n",
    "\n",
    "# 학습 기록\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_ppl': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_ppl': []\n",
    "}\n",
    "\n",
    "print(f\"학습 시작!\")\n",
    "print(f\"   - 에폭 수: {config.max_epoch}\")\n",
    "print(f\"   - 배치 크기: {config.batch_size}\")\n",
    "print(f\"   - 이터레이션/에폭: {max_iters:,}\")\n",
    "print(f\"   - Optimizer: Adam (lr={config.learning_rate})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "best_val_ppl = float('inf')\n",
    "\n",
    "for epoch in range(config.max_epoch):\n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for iter_idx in range(max_iters):\n",
    "        # 미니배치 생성\n",
    "        time_idx = iter_idx * config.time_size\n",
    "        inputs, targets = get_batch(\n",
    "            corpus_train, config.batch_size, config.time_size,\n",
    "            time_idx, offsets, data_size\n",
    "        )\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Truncated BPTT: 은닉 상태 분리\n",
    "        if hidden is not None:\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "        \n",
    "        # 순전파\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 에폭 통계\n",
    "    train_loss = total_loss / max_iters\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    \n",
    "    # 검증\n",
    "    valid_loss = evaluate(model, corpus_valid, config.batch_size, config.time_size, criterion)\n",
    "    valid_ppl = np.exp(valid_loss)\n",
    "    \n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step(valid_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # 기록\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['valid_ppl'].append(valid_ppl)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        f\"| Epoch {epoch+1:2d}/{config.max_epoch} \"\n",
    "        f\"| Train PPL {train_ppl:8.2f} \"\n",
    "        f\"| Valid PPL {valid_ppl:8.2f} \"\n",
    "        f\"| LR {current_lr:.6f} | Time {elapsed:5.0f}s |\"\n",
    "    )\n",
    "    \n",
    "    # 최적 모델 저장\n",
    "    if valid_ppl < best_val_ppl:\n",
    "        best_val_ppl = valid_ppl\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'best_val_ppl': best_val_ppl,\n",
    "            'config': {\n",
    "                'vocab_size': vocab_size,\n",
    "                'embed_size': config.embed_size,\n",
    "                'hidden_size': config.hidden_size,\n",
    "                'num_layers': config.num_layers\n",
    "            }\n",
    "        }, 'best_lstm_lm.pth')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"학습 완료! 총 소요 시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"최종 Train PPL: {train_ppl:.2f}\")\n",
    "print(f\"최종 Valid PPL: {valid_ppl:.2f}\")\n",
    "print(f\"최적 Valid PPL: {best_val_ppl:.2f}\")\n",
    "\n",
    "# 최적 모델 로드\n",
    "checkpoint = torch.load('best_lstm_lm.pth', weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"최적 모델 로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_path = 'lstm_lm.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_size': config.embed_size,\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'num_layers': config.num_layers\n",
    "    },\n",
    "    'history': history\n",
    "}, model_path)\n",
    "print(f\"모델 저장 완료: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 시각화\n",
    "\n",
    "학습 과정에서의 손실(Loss)과 Perplexity(PPL) 변화를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss 그래프\n",
    "ax1 = axes[0]\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, history['valid_loss'], 'r-s', label='Valid Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PPL 그래프\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, history['train_ppl'], 'b-o', label='Train PPL', linewidth=2)\n",
    "ax2.plot(epochs, history['valid_ppl'], 'r-s', label='Valid PPL', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Perplexity', fontsize=12)\n",
    "ax2.set_title('Training & Validation Perplexity', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n최종 결과:\")\n",
    "print(f\"   - Train PPL: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"   - Valid PPL: {history['valid_ppl'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 텍스트 생성\n",
    "\n",
    "학습된 LSTM 모델을 사용하여 텍스트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: nn.Module,\n",
    "    start_word: str,\n",
    "    word_to_id: Dict[str, int],\n",
    "    id_to_word: Dict[int, str],\n",
    "    length: int = 50,\n",
    "    temperature: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    학습된 모델로 텍스트를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: 학습된 LSTM 모델\n",
    "        start_word: 시작 단어\n",
    "        word_to_id: 단어 → ID 매핑\n",
    "        id_to_word: ID → 단어 매핑\n",
    "        length: 생성할 단어 수\n",
    "        temperature: 샘플링 온도 (높을수록 다양함)\n",
    "        \n",
    "    Returns:\n",
    "        생성된 텍스트\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if start_word.lower() not in word_to_id:\n",
    "        return f\"'{start_word}'는 어휘에 없는 단어입니다.\"\n",
    "    \n",
    "    input_id = torch.tensor([[word_to_id[start_word.lower()]]], device=device)\n",
    "    hidden = None\n",
    "    result = [start_word.lower()]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_id, hidden)\n",
    "            \n",
    "            logits = output.squeeze() / temperature\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "            \n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            result.append(id_to_word[next_id])\n",
    "            \n",
    "            input_id = torch.tensor([[next_id]], device=device)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "print(\"\\n생성된 텍스트:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for start_word in ['the', 'in', 'he', 'it']:\n",
    "    if start_word in word_to_id:\n",
    "        generated = generate_text(\n",
    "            model, start_word, word_to_id, id_to_word,\n",
    "            length=30, temperature=0.8\n",
    "        )\n",
    "        print(f\"\\n[{start_word}]로 시작:\")\n",
    "        print(f\"  {generated}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature에 따른 생성 결과 비교\n",
    "print(\"\\nTemperature에 따른 생성 결과 비교:\\n\")\n",
    "\n",
    "start = 'the'\n",
    "for temp in [0.5, 0.8, 1.0, 1.2]:\n",
    "    text = generate_text(\n",
    "        model, start, word_to_id, id_to_word,\n",
    "        length=20, temperature=temp\n",
    "    )\n",
    "    print(f\"Temperature={temp}: {text}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
