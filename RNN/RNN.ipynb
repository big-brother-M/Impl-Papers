{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# íŒŒì´í† ì¹˜(PyTorch)ë¡œ êµ¬í˜„í•˜ëŠ” Mikolov 2010 RNN ì–¸ì–´ ëª¨ë¸ (RNNLM)\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ **\"Recurrent neural network based language model\" (Mikolov et al., 2010)** ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœ RNN(Simple RNN)ì„ êµ¬í˜„í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ë…¼ë¬¸ì€ í˜„ëŒ€ì ì¸ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ë§ì˜ ì‹œì´ˆê°€ ëœ ì¤‘ìš”í•œ ì—°êµ¬ë¡œ, ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ RNN êµ¬ì¡°ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ëª©ì°¨\n",
    "\n",
    "1. [ëª¨ë¸ êµ¬ì¡° ë° ì´ë¡ ](#1-mikolov-rnnlm-ëª¨ë¸-êµ¬ì¡°)\n",
    "2. [í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬](#2-í™˜ê²½-ì„¤ì •)\n",
    "3. [ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬](#3-ë°ì´í„°-ë¡œë“œ-ë°-ì „ì²˜ë¦¬)\n",
    "4. [RNN ëª¨ë¸ êµ¬í˜„](#4-sigmoid-rnn-êµ¬í˜„)\n",
    "5. [í•™ìŠµ](#5-í•™ìŠµ-training)\n",
    "6. [ì‹œê°í™” ë° ê²°ê³¼ ë¶„ì„](#6-ê²°ê³¼-ì‹œê°í™”)\n",
    "7. [í…ìŠ¤íŠ¸ ìƒì„±](#7-í…ìŠ¤íŠ¸-ìƒì„±)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mikolov RNNLM ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### 1.1 ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
    "\n",
    "ëª¨ë¸ì€ í¬ê²Œ **ì…ë ¥ì¸µ(Input)**, **ì€ë‹‰ì¸µ(Hidden)**, **ì¶œë ¥ì¸µ(Output)** ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Input (w)  â”‚ â”€â”€â–¶ â”‚ Hidden (s)  â”‚ â”€â”€â–¶ â”‚ Output (y)  â”‚\n",
    "â”‚  (One-hot)  â”‚     â”‚  (Sigmoid)  â”‚     â”‚  (Softmax)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ Recurrent    â”‚\n",
    "                    â”‚ Connection   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 1.2 í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "**ì€ë‹‰ ìƒíƒœ ì—…ë°ì´íŠ¸:**\n",
    "$$ s(t) = \\sigma(U \\cdot w(t) + W \\cdot s(t-1)) $$\n",
    "\n",
    "**ì¶œë ¥ ê³„ì‚°:**\n",
    "$$ y(t) = \\text{softmax}(V \\cdot s(t)) $$\n",
    "\n",
    "### 1.3 ë³€ìˆ˜ ì„¤ëª…\n",
    "\n",
    "| ê¸°í˜¸ | ì„¤ëª… | ì°¨ì› |\n",
    "|:---:|:---|:---|\n",
    "| $w(t)$ | ì‹œê°„ $t$ì—ì„œì˜ ì…ë ¥ ë‹¨ì–´ ë²¡í„° | $(V, 1)$ |\n",
    "| $s(t)$ | ì‹œê°„ $t$ì—ì„œì˜ ì€ë‹‰ ìƒíƒœ | $(H, 1)$ |\n",
    "| $y(t)$ | ì‹œê°„ $t$ì—ì„œì˜ ì¶œë ¥ í™•ë¥  ë¶„í¬ | $(V, 1)$ |\n",
    "| $U$ | ì…ë ¥ â†’ ì€ë‹‰ ê°€ì¤‘ì¹˜ | $(H, V)$ |\n",
    "| $W$ | ì€ë‹‰ â†’ ì€ë‹‰ ê°€ì¤‘ì¹˜ (ìˆœí™˜ ì—°ê²°) | $(H, H)$ |\n",
    "| $V$ | ì€ë‹‰ â†’ ì¶œë ¥ ê°€ì¤‘ì¹˜ | $(V, H)$ |\n",
    "\n",
    "### 1.4 ì™œ Sigmoidì¸ê°€?\n",
    "\n",
    "PyTorchì˜ ê¸°ë³¸ `nn.RNN`ì€ **Tanh** í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, **Mikolovì˜ ì›ë…¼ë¬¸ì—ì„œëŠ” Sigmoid**ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **Sigmoid**: $\\sigma(x) = \\frac{1}{1+e^{-x}}$, ì¶œë ¥ ë²”ìœ„ $(0, 1)$\n",
    "- **Tanh**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$, ì¶œë ¥ ë²”ìœ„ $(-1, 1)$\n",
    "\n",
    "ë³¸ êµ¬í˜„ì—ì„œëŠ” ë…¼ë¬¸ì˜ ìŠ¤í™ì„ ì •í™•íˆ ë”°ë¥´ê¸° ìœ„í•´ **Custom Sigmoid RNN Cell**ì„ ì§ì ‘ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device ì„¤ì • (CUDA > MPS > CPU ìš°ì„ ìˆœìœ„)\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë””ë°”ì´ìŠ¤ ë°˜í™˜\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"ì‚¬ìš© ì¥ì¹˜(Device): {device}\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"ëª¨ë¸ ë° í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\"\"\"\n",
    "    \n",
    "    # ë°ì´í„° ì„¤ì •\n",
    "    dataset_name: str = \"Salesforce/wikitext\"\n",
    "    dataset_config: str = \"wikitext-2-raw-v1\"\n",
    "    min_count: int = 5  # ìµœì†Œ ì¶œí˜„ ë¹ˆë„\n",
    "    \n",
    "    # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë…¼ë¬¸: Hidden Layer 30~500)\n",
    "    embed_size: int = 100   # ì„ë² ë”© ì°¨ì›\n",
    "    hidden_size: int = 100  # ì€ë‹‰ ìƒíƒœ ì°¨ì›\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    batch_size: int = 32\n",
    "    time_size: int = 35     # Truncated BPTT ê¸¸ì´\n",
    "    max_epoch: int = 10\n",
    "    learning_rate: float = 0.1  # SGD í•™ìŠµë¥ \n",
    "    clip_grad: float = 0.25     # Gradient Clipping ì„ê³„ê°’\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Config(\\n\"\n",
    "            f\"  embed_size={self.embed_size}, hidden_size={self.hidden_size},\\n\"\n",
    "            f\"  batch_size={self.batch_size}, time_size={self.time_size},\\n\"\n",
    "            f\"  max_epoch={self.max_epoch}, lr={self.learning_rate}\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "config = Config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "**Wikitext-2** ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì¶”ì¶œëœ ì–‘ì§ˆì˜ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì–¸ì–´ ëª¨ë¸ë§ ë²¤ì¹˜ë§ˆí¬ë¡œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "### ì „ì²˜ë¦¬ ê³¼ì •:\n",
    "1. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ\n",
    "2. ë‹¨ì–´ ë¹ˆë„ ì¹´ìš´íŒ…\n",
    "3. ìµœì†Œ ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
    "4. ë‹¨ì–´ â†’ ID ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì¤‘...\")\n",
    "ds = load_dataset(config.dataset_name, config.dataset_config)\n",
    "print(f\"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   - Train: {len(ds['train'])} ë¬¸ì¥\")\n",
    "print(f\"   - Validation: {len(ds['validation'])} ë¬¸ì¥\")\n",
    "print(f\"   - Test: {len(ds['test'])} ë¬¸ì¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(\n",
    "    dataset, \n",
    "    splits: List[str] = ['train', 'validation', 'test'],\n",
    "    min_count: int = 5\n",
    ") -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì—ì„œ ë‹¨ì–´ ë¹ˆë„ë¥¼ ì¹´ìš´íŒ…í•˜ê³  ì–´íœ˜ ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace ë°ì´í„°ì…‹\n",
    "        splits: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ë¶„í• \n",
    "        min_count: ìµœì†Œ ì¶œí˜„ ë¹ˆë„\n",
    "        \n",
    "    Returns:\n",
    "        word_to_id: ë‹¨ì–´ â†’ ID ë§¤í•‘\n",
    "        id_to_word: ID â†’ ë‹¨ì–´ ë§¤í•‘\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    for split in splits:\n",
    "        for line in dataset[split]['text']:\n",
    "            words = line.strip().lower().split()\n",
    "            counter.update(words)\n",
    "    \n",
    "    # <unk> í† í°ì€ í•­ìƒ 0ë²ˆ ID\n",
    "    word_to_id = {'<unk>': 0}\n",
    "    id_to_word = {0: '<unk>'}\n",
    "    \n",
    "    # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ID ë¶€ì—¬\n",
    "    sorted_words = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    valid_words = [word for word, count in sorted_words if count >= min_count]\n",
    "    \n",
    "    for word in valid_words:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "# ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
    "word_to_id, id_to_word = build_vocab(ds, min_count=config.min_count)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "print(f\"\\n ì–´íœ˜ ì‚¬ì „ í†µê³„:\")\n",
    "print(f\"   - ì „ì²´ ì–´íœ˜ í¬ê¸°: {vocab_size:,}\")\n",
    "print(f\"   - ìµœì†Œ ì¶œí˜„ ë¹ˆë„: {config.min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(\n",
    "    dataset, \n",
    "    split: str, \n",
    "    word_to_id: Dict[str, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¨ì–´ ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace ë°ì´í„°ì…‹\n",
    "        split: ë°ì´í„° ë¶„í•  ('train', 'validation', 'test')\n",
    "        word_to_id: ë‹¨ì–´ â†’ ID ë§¤í•‘\n",
    "        \n",
    "    Returns:\n",
    "        ids: ë‹¨ì–´ ID ë°°ì—´\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    unk_id = word_to_id['<unk>']\n",
    "    \n",
    "    for line in dataset[split]['text']:\n",
    "        words = line.strip().lower().split()\n",
    "        if not words:\n",
    "            continue\n",
    "        ids.extend([word_to_id.get(w, unk_id) for w in words])\n",
    "    \n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "# ê° ë¶„í• ì— ëŒ€í•´ ID ë³€í™˜\n",
    "corpus_train = convert_to_ids(ds, 'train', word_to_id)\n",
    "corpus_valid = convert_to_ids(ds, 'validation', word_to_id)\n",
    "corpus_test = convert_to_ids(ds, 'test', word_to_id)\n",
    "\n",
    "print(f\"\\n ì½”í¼ìŠ¤ í¬ê¸°:\")\n",
    "print(f\"   - Train: {len(corpus_train):,} tokens\")\n",
    "print(f\"   - Validation: {len(corpus_valid):,} tokens\")\n",
    "print(f\"   - Test: {len(corpus_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sigmoid RNN êµ¬í˜„\n",
    "\n",
    "PyTorchì˜ `nn.RNN`ì€ ê¸°ë³¸ì ìœ¼ë¡œ `tanh` í˜¹ì€ `relu`ë§Œ ì§€ì›í•©ë‹ˆë‹¤. Mikolov ë…¼ë¬¸ì˜ ìŠ¤í™ì„ ë”°ë¥´ê¸° ìœ„í•´ **Sigmoid í™œì„±í™” í•¨ìˆ˜**ë¥¼ ì‚¬ìš©í•˜ëŠ” RNN Layerë¥¼ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### êµ¬í˜„ ìˆ˜ì‹\n",
    "\n",
    "$$h_t = \\sigma(x_t W_{ih}^T + b_{ih} + h_{t-1} W_{hh}^T + b_{hh})$$\n",
    "\n",
    "ì—¬ê¸°ì„œ:\n",
    "- $x_t$: ì‹œê°„ $t$ì˜ ì…ë ¥ (ì„ë² ë”© ë²¡í„°)\n",
    "- $h_{t-1}$: ì´ì „ ì‹œê°„ì˜ ì€ë‹‰ ìƒíƒœ\n",
    "- $W_{ih}$, $W_{hh}$: ê°€ì¤‘ì¹˜ í–‰ë ¬\n",
    "- $b_{ih}$, $b_{hh}$: í¸í–¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MikolovRNNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mikolov ë…¼ë¬¸ ìŠ¤í™ì— ë§ì¶˜ Sigmoid RNN Layer.\n",
    "    \n",
    "    ì¼ë°˜ì ì¸ RNNê³¼ ë‹¬ë¦¬ Sigmoid í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        input_size: ì…ë ¥ ì°¨ì›\n",
    "        hidden_size: ì€ë‹‰ ìƒíƒœ ì°¨ì›\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # ì…ë ¥ â†’ ì€ë‹‰ ë³€í™˜ (U í–‰ë ¬ì— í•´ë‹¹)\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        # ì€ë‹‰ â†’ ì€ë‹‰ ë³€í™˜ (W í–‰ë ¬ì— í•´ë‹¹, ìˆœí™˜ ì—°ê²°)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        # Sigmoid í™œì„±í™” í•¨ìˆ˜\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        h_prev: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        ìˆœì „íŒŒ ê³„ì‚°.\n",
    "        \n",
    "        Args:\n",
    "            x: ì…ë ¥ í…ì„œ (batch, time_size, input_size)\n",
    "            h_prev: ì´ˆê¸° ì€ë‹‰ ìƒíƒœ (batch, hidden_size), Noneì´ë©´ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "            \n",
    "        Returns:\n",
    "            outputs: ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì˜ ì€ë‹‰ ìƒíƒœ (batch, time_size, hidden_size)\n",
    "            h_last: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, time_size, _ = x.shape\n",
    "        \n",
    "        # ì´ˆê¸° ì€ë‹‰ ìƒíƒœê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        h = h_prev\n",
    "        outputs = []\n",
    "        \n",
    "        # ê° ì‹œê°„ ë‹¨ê³„ë³„ë¡œ ìˆœì°¨ ê³„ì‚°\n",
    "        for t in range(time_size):\n",
    "            x_t = x[:, t, :]\n",
    "            # h_t = sigmoid(U * x_t + W * h_{t-1})\n",
    "            h = self.activation(self.i2h(x_t) + self.h2h(h))\n",
    "            outputs.append(h.unsqueeze(1))\n",
    "        \n",
    "        # ì „ì²´ ì‹œí€€ìŠ¤ ì¶œë ¥: (batch, time, hidden)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return outputs, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MikolovRNNLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Mikolov 2010 RNN ì–¸ì–´ ëª¨ë¸.\n",
    "    \n",
    "    êµ¬ì¡°: Embedding â†’ Sigmoid RNN â†’ Linear â†’ Softmax\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: ì–´íœ˜ í¬ê¸°\n",
    "        embed_size: ì„ë² ë”© ì°¨ì›\n",
    "        hidden_size: ì€ë‹‰ ìƒíƒœ ì°¨ì›\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 1. ì…ë ¥ì¸µ (Embedding)\n",
    "        # ë…¼ë¬¸ì˜ One-hot ì…ë ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„ (ìˆ˜í•™ì ìœ¼ë¡œ ë™ë“±)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # 2. ì€ë‹‰ì¸µ (Sigmoid RNN)\n",
    "        self.rnn = MikolovRNNLayer(embed_size, hidden_size)\n",
    "        \n",
    "        # 3. ì¶œë ¥ì¸µ (Linear â†’ SoftmaxëŠ” CrossEntropyLossì—ì„œ ì²˜ë¦¬)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self, init_range: float = 0.1):\n",
    "        \"\"\"ê°€ì¤‘ì¹˜ë¥¼ ê· ë“± ë¶„í¬ë¡œ ì´ˆê¸°í™”\"\"\"\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        h: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        ìˆœì „íŒŒ.\n",
    "        \n",
    "        Args:\n",
    "            x: ì…ë ¥ ë‹¨ì–´ ID (batch, time_size)\n",
    "            h: ì´ˆê¸° ì€ë‹‰ ìƒíƒœ\n",
    "            \n",
    "        Returns:\n",
    "            out: ë¡œì§“ (batch, time_size, vocab_size)\n",
    "            h_new: ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœ\n",
    "        \"\"\"\n",
    "        # Embedding Lookup\n",
    "        embeds = self.embedding(x)  # (batch, time, embed_size)\n",
    "        \n",
    "        # RNN ê³„ì‚°\n",
    "        out, h_new = self.rnn(embeds, h)  # (batch, time, hidden_size)\n",
    "        \n",
    "        # ì¶œë ¥ì¸µ\n",
    "        out = self.fc(out)  # (batch, time, vocab_size)\n",
    "        \n",
    "        return out, h_new\n",
    "    \n",
    "    def get_num_params(self) -> int:\n",
    "        \"\"\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„± ë° ì •ë³´ ì¶œë ¥\n",
    "model = MikolovRNNLM(vocab_size, config.embed_size, config.hidden_size).to(device)\n",
    "\n",
    "print(\"ëª¨ë¸ êµ¬ì¡°:\")\n",
    "print(model)\n",
    "print(f\"\\n ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {model.get_num_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ (Training)\n",
    "\n",
    "### 5.1 Truncated BPTT (Backpropagation Through Time)\n",
    "\n",
    "RNNì˜ ì—­ì „íŒŒëŠ” ì‹œê°„ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ BPTTë¥¼ ìˆ˜í–‰í•˜ë©´ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë¹„ìš©ì´ í¬ë¯€ë¡œ, **Truncated BPTT**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "ì „ì²´ ì‹œí€€ìŠ¤:  [w1, w2, w3, w4, w5, w6, w7, w8, ...]\n",
    "                    â”‚         â”‚         â”‚\n",
    "Truncated:   [chunk1]   [chunk2]   [chunk3]  ...\n",
    "```\n",
    "\n",
    "### 5.2 í•™ìŠµ ì„¤ì •\n",
    "\n",
    "- **Optimizer**: SGD (ë…¼ë¬¸ ìŠ¤í™)\n",
    "- **Loss**: Cross-Entropy\n",
    "- **Gradient Clipping**: ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    corpus: np.ndarray,\n",
    "    batch_size: int,\n",
    "    time_size: int,\n",
    "    time_idx: int,\n",
    "    offsets: List[int],\n",
    "    data_size: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    í•™ìŠµìš© ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        corpus: ì „ì²´ ì½”í¼ìŠ¤ (ë‹¨ì–´ ID ë°°ì—´)\n",
    "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
    "        time_size: ì‹œí€€ìŠ¤ ê¸¸ì´ (BPTT ê¸¸ì´)\n",
    "        time_idx: í˜„ì¬ ì‹œê°„ ì¸ë±ìŠ¤\n",
    "        offsets: ê° ë°°ì¹˜ ìƒ˜í”Œì˜ ì‹œì‘ ì˜¤í”„ì…‹\n",
    "        data_size: ì½”í¼ìŠ¤ í¬ê¸°\n",
    "        \n",
    "    Returns:\n",
    "        inputs: ì…ë ¥ ì‹œí€€ìŠ¤ (batch, time_size)\n",
    "        targets: íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (batch, time_size)\n",
    "    \"\"\"\n",
    "    batch_x, batch_t = [], []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        ptr = (offsets[i] + time_idx) % data_size\n",
    "        indices = [(ptr + t) % data_size for t in range(time_size + 1)]\n",
    "        raw_data = corpus[indices]\n",
    "        batch_x.append(raw_data[:-1])   # ì…ë ¥: 0 ~ time_size-1\n",
    "        batch_t.append(raw_data[1:])    # íƒ€ê²Ÿ: 1 ~ time_size (ë‹¤ìŒ ë‹¨ì–´)\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(np.array(batch_x), dtype=torch.long),\n",
    "        torch.tensor(np.array(batch_t), dtype=torch.long)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    corpus: np.ndarray,\n",
    "    batch_size: int,\n",
    "    time_size: int,\n",
    "    criterion: nn.Module\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì„ í‰ê°€í•˜ê³  í‰ê·  ì†ì‹¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        model: RNN ëª¨ë¸\n",
    "        corpus: í‰ê°€ìš© ì½”í¼ìŠ¤\n",
    "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
    "        time_size: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        criterion: ì†ì‹¤ í•¨ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        avg_loss: í‰ê·  ì†ì‹¤\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_size = len(corpus)\n",
    "    max_iters = data_size // (batch_size * time_size)\n",
    "    jump = (data_size - 1) // batch_size\n",
    "    offsets = [i * jump for i in range(batch_size)]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iter_idx in range(max_iters):\n",
    "            inputs, targets = get_batch(\n",
    "                corpus, batch_size, time_size, \n",
    "                iter_idx * time_size, offsets, data_size\n",
    "            )\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, model.vocab_size),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if hidden is not None:\n",
    "                hidden = hidden.detach()\n",
    "    \n",
    "    return total_loss / max_iters if max_iters > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì„¤ì •\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# ë°°ì¹˜ ìƒ˜í”Œë§ì„ ìœ„í•œ ì˜¤í”„ì…‹ ê³„ì‚°\n",
    "data_size = len(corpus_train)\n",
    "max_iters = data_size // (config.batch_size * config.time_size)\n",
    "jump = (data_size - 1) // config.batch_size\n",
    "offsets = [i * jump for i in range(config.batch_size)]\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_ppl': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_ppl': []\n",
    "}\n",
    "\n",
    "print(f\"\\n í•™ìŠµ ì‹œì‘!\")\n",
    "print(f\"   - ì—í­ ìˆ˜: {config.max_epoch}\")\n",
    "print(f\"   - ë°°ì¹˜ í¬ê¸°: {config.batch_size}\")\n",
    "print(f\"   - ì´í„°ë ˆì´ì…˜/ì—í­: {max_iters:,}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.max_epoch):\n",
    "    model.train()\n",
    "    hidden = None\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for iter_idx in range(max_iters):\n",
    "        # ë¯¸ë‹ˆë°°ì¹˜ ìƒì„±\n",
    "        time_idx = iter_idx * config.time_size\n",
    "        inputs, targets = get_batch(\n",
    "            corpus_train, config.batch_size, config.time_size,\n",
    "            time_idx, offsets, data_size\n",
    "        )\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Truncated BPTT: ì´ì „ ì€ë‹‰ ìƒíƒœ ë¶„ë¦¬\n",
    "        if hidden is not None:\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        # ìˆœì „íŒŒ\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # ì—í­ í†µê³„\n",
    "    train_loss = total_loss / max_iters\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    \n",
    "    # ê²€ì¦ ì„¸íŠ¸ í‰ê°€\n",
    "    valid_loss = evaluate(\n",
    "        model, corpus_valid, config.batch_size, \n",
    "        config.time_size, criterion\n",
    "    )\n",
    "    valid_ppl = np.exp(valid_loss)\n",
    "    \n",
    "    # ê¸°ë¡ ì €ì¥\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['valid_ppl'].append(valid_ppl)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        f\"| Epoch {epoch+1:2d}/{config.max_epoch} \"\n",
    "        f\"| Train Loss {train_loss:.4f} | Train PPL {train_ppl:8.2f} \"\n",
    "        f\"| Valid Loss {valid_loss:.4f} | Valid PPL {valid_ppl:8.2f} \"\n",
    "        f\"| Time {elapsed:6.1f}s |\"\n",
    "    )\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "model_path = 'mikolov_rnn.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_size': config.embed_size,\n",
    "        'hidden_size': config.hidden_size\n",
    "    },\n",
    "    'history': history\n",
    "}, model_path)\n",
    "print(f\" ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "í•™ìŠµ ê³¼ì •ì—ì„œì˜ ì†ì‹¤(Loss)ê³¼ Perplexity(PPL) ë³€í™”ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Perplexity (PPL)ë€?\n",
    "\n",
    "PerplexityëŠ” ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ, ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œì˜ \"í˜¼ë€ë„\"ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "$$PPL = e^{\\text{Cross-Entropy Loss}}$$\n",
    "\n",
    "- **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ**: ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë” ì˜ ì˜ˆì¸¡\n",
    "- **í•´ì„**: PPL=100ì´ë©´, ëª¨ë¸ì´ ê° ìœ„ì¹˜ì—ì„œ í‰ê·  100ê°œì˜ ë‹¨ì–´ ì¤‘ì—ì„œ ê³ ë¥´ëŠ” ê²ƒì²˜ëŸ¼ \"í˜¼ë€\"í•´ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss ê·¸ë˜í”„\n",
    "ax1 = axes[0]\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)\n",
    "ax1.plot(epochs, history['valid_loss'], 'r-s', label='Valid Loss', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(epochs)\n",
    "\n",
    "# PPL ê·¸ë˜í”„\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, history['train_ppl'], 'b-o', label='Train PPL', linewidth=2, markersize=6)\n",
    "ax2.plot(epochs, history['valid_ppl'], 'r-s', label='Valid PPL', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Perplexity', fontsize=12)\n",
    "ax2.set_title('Training & Validation Perplexity', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(epochs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n ìµœì¢… ê²°ê³¼:\")\n",
    "print(f\"   - Train PPL: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"   - Valid PPL: {history['valid_ppl'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í…ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. **Temperature** íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±ì˜ ë‹¤ì–‘ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **Temperature = 1.0**: ì›ë˜ í™•ë¥  ë¶„í¬ ì‚¬ìš©\n",
    "- **Temperature < 1.0**: ë” í™•ì‹¤í•œ (ê³ ë¹ˆë„) ë‹¨ì–´ ì„ í˜¸\n",
    "- **Temperature > 1.0**: ë” ë‹¤ì–‘í•œ (ì €ë¹ˆë„ í¬í•¨) ë‹¨ì–´ í—ˆìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: nn.Module,\n",
    "    start_word: str,\n",
    "    word_to_id: Dict[str, int],\n",
    "    id_to_word: Dict[int, str],\n",
    "    length: int = 50,\n",
    "    temperature: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµëœ RNN ëª¨ë¸\n",
    "        start_word: ì‹œì‘ ë‹¨ì–´\n",
    "        word_to_id: ë‹¨ì–´ â†’ ID ë§¤í•‘\n",
    "        id_to_word: ID â†’ ë‹¨ì–´ ë§¤í•‘\n",
    "        length: ìƒì„±í•  ë‹¨ì–´ ìˆ˜\n",
    "        temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)\n",
    "        \n",
    "    Returns:\n",
    "        ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ì‹œì‘ ë‹¨ì–´ í™•ì¸\n",
    "    if start_word.lower() not in word_to_id:\n",
    "        return f\"'{start_word}'ëŠ” ì–´íœ˜ì— ì—†ëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤.\"\n",
    "    \n",
    "    # ì´ˆê¸° ìƒíƒœ ì„¤ì •\n",
    "    input_id = torch.tensor(\n",
    "        [[word_to_id[start_word.lower()]]], \n",
    "        device=device\n",
    "    )\n",
    "    hidden = None\n",
    "    result = [start_word.lower()]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # ìˆœì „íŒŒ\n",
    "            output, hidden = model(input_id, hidden)\n",
    "            \n",
    "            # Temperature ì ìš© ë° í™•ë¥  ê³„ì‚°\n",
    "            logits = output.squeeze() / temperature\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "            \n",
    "            # í™•ë¥ ì  ìƒ˜í”Œë§\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            result.append(id_to_word[next_id])\n",
    "            \n",
    "            # ë‹¤ìŒ ì…ë ¥ ì¤€ë¹„\n",
    "            input_id = torch.tensor([[next_id]], device=device)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ\n",
    "print(\"\\n ìƒì„±ëœ í…ìŠ¤íŠ¸:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for start_word in ['the', 'in', 'he', 'it']:\n",
    "    if start_word in word_to_id:\n",
    "        generated = generate_text(\n",
    "            model, start_word, word_to_id, id_to_word,\n",
    "            length=30, temperature=0.8\n",
    "        )\n",
    "        print(f\"\\n[{start_word}]ë¡œ ì‹œì‘:\")\n",
    "        print(f\"  {generated}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperatureì— ë”°ë¥¸ ìƒì„± ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n Temperatureì— ë”°ë¥¸ ìƒì„± ê²°ê³¼ ë¹„êµ:\\n\")\n",
    "\n",
    "start = 'the'\n",
    "for temp in [0.5, 0.8, 1.0, 1.2]:\n",
    "    text = generate_text(\n",
    "        model, start, word_to_id, id_to_word,\n",
    "        length=20, temperature=temp\n",
    "    )\n",
    "    print(f\"Temperature={temp}: {text}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
